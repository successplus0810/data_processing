{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join, abspath\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "# from azure.storage.blob import BlobServiceClient\n",
    "from azure.storage.blob import ContainerCliente\n",
    "import snowflake.connector as sf\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import win32com.client as win32\n",
    "\n",
    "# 1. Analyst Input info  ######################################################################\n",
    "weekly_record_move_file = \n",
    "\n",
    "def upload_SF(df_tracker):\n",
    "    \"\"\"Set up connection to SnowFlake\"\"\"\n",
    "    # account = \"ih73640.australia-east.azure\"\n",
    "    # user = \"duongnguyen@profectusgroup.com\"\n",
    "    # warehouse = \"W1\"\n",
    "    # role = \"PVN_AUDIT_COLES\"\n",
    "#################################################################################################\n",
    "    account = \"ih73640.australia-east.azure\"\n",
    "    user = \"tdinh@profectusgroup.com\"\n",
    "    warehouse = \"W1\"\n",
    "    role = \"PVN_AUDIT_LEADS\"\n",
    "    database = \"DEMO_DB\"\n",
    "    schema = \"CATALOGUE\"\n",
    "    password = \"\"\n",
    "    auth = \"externalbrowser\"\n",
    "\n",
    "    conn = sf.connect(user=user, password=password, account=account, authenticator=auth,\n",
    "                      warehouse=warehouse, role=role, database=database, schema=schema)\n",
    "    print('start loading data')\n",
    "    success, nchunks ,nrows , _ = write_pandas(conn,df_tracker,'CATALOGUE_TRACKING',compression = 'gzip')\n",
    "    print(str(success)+', '+str(nchunks)+', '+str(nrows))\n",
    "    print('done loading data')\n",
    "    conn.close()\n",
    "    return None\n",
    "\n",
    "container_client = ContainerClient.from_container_url(sas_url)\n",
    "\n",
    "def find_path_week():\n",
    "    today = datetime.date.today()\n",
    "    if (today.weekday() == 0):\n",
    "        monday = today + datetime.timedelta( (7-today.weekday()) % 7)\n",
    "    else:\n",
    "        monday = today + datetime.timedelta( (7-today.weekday()) % 7 -7)\n",
    "    cata_date = monday.strftime(\"%Y%m%d\")\n",
    "    cata_date_year = cata_date[0:4]\n",
    "    cata_date_month = cata_date[4:6]\n",
    "    cata_date_day = cata_date[2:]\n",
    "    return fr'D:\\OneDrive - Profectus Group\\Ongoing Catalogue\\Auto\\Tool\\catalogues\\{cata_date_year}\\{cata_date_month}\\{cata_date_day}'\n",
    "\n",
    "\n",
    "\n",
    "path_week = find_path_week()\n",
    "cat_date= '20' + path_week.split('\\\\')[-1]\n",
    "cat_year = cat_date[0:4]\n",
    "\n",
    "# list all file in folder\n",
    "dict_ven_filename = {}\n",
    "for (dir_path, dir_names, file_names) in os.walk(path_week):\n",
    "    for file_name in file_names:\n",
    "        vendor_name = file_name.split('_')[0]\n",
    "        dict_ven_filename[join(path_week,file_name)] = vendor_name\n",
    "\n",
    "df_vendor_transfer = pd.read_excel(r'vendor_transfer.xlsx')\n",
    "#transfer link\n",
    "def transfer_link_sharepoint(df_vendor_transfer,dict_ven_filename):\n",
    "    output_path = join('D:\\OneDrive - Profectus Group\\Ongoing Catalogue','vendor_transfer',cat_year,cat_date)\n",
    "    dict_vendor_transfer = df_vendor_transfer.set_index('Retailer_name')['Vendor_path'].to_dict()\n",
    "    dict_link_transfer = {}\n",
    "    for file_name , vendor_name in dict_ven_filename.items():\n",
    "        if vendor_name in dict_vendor_transfer.keys():\n",
    "            dict_link_transfer[file_name] = output_path.replace('vendor_transfer',dict_vendor_transfer[vendor_name])\n",
    "    return dict_link_transfer\n",
    "\n",
    "\n",
    "def transfer_link_azure_storage(df_vendor_transfer,dict_ven_filename):\n",
    "    output_path = join('vendor_transfer',cat_year,cat_date).replace('\\\\','/')\n",
    "    dict_vendor_transfer = df_vendor_transfer.set_index('Retailer_name')['Vendor_path'].to_dict()\n",
    "    dict_link_transfer = {}\n",
    "    for file_name , vendor_name in dict_ven_filename.items():\n",
    "        if vendor_name in dict_vendor_transfer.keys():\n",
    "            dict_link_transfer[file_name] = output_path.replace('vendor_transfer',dict_vendor_transfer[vendor_name])\n",
    "    return dict_link_transfer\n",
    "\n",
    "def copy_file_sharepoint(dict_link_transfer_sharepoint):\n",
    "    with open(weekly_record_move_file, \"a\", encoding=\"utf-8\", newline=\"\") as weekly_record_io:\n",
    "        weekly_record_writer = csv.writer(weekly_record_io)     \n",
    "        for source_path, des_path in dict_link_transfer_sharepoint.items():\n",
    "            # print(source_path, des_path)\n",
    "            Path(des_path).mkdir(parents=True, exist_ok=True)\n",
    "            if (isfile(join(des_path,source_path.split('\\\\')[-1])) == False):\n",
    "                des_path = join(des_path,source_path.split('\\\\')[-1])\n",
    "                print(source_path, des_path)\n",
    "                # print('new')\n",
    "                try:\n",
    "                    shutil.copy2(source_path,des_path)\n",
    "                    status = 'Done'\n",
    "                except:\n",
    "                    status = 'Fail'\n",
    "                now_dtt = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "                weekly_record_writer.writerow([path_week.split('\\\\')[-1],source_path,des_path,status, now_dtt])\n",
    "    return True\n",
    "\n",
    "def copy_file_azure(dict_link_transfer_azure):\n",
    "    for source_path, des_path in dict_link_transfer_azure.items():\n",
    "        des_path = join(des_path,source_path.split('\\\\')[-1])\n",
    "        # Open the file in read mode\n",
    "        print(source_path,des_path)\n",
    "        with open(source_path, 'rb') as data:\n",
    "            # Get a reference to the blob\n",
    "            blob_client = container_client.get_blob_client(des_path)\n",
    "            # Upload the file\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f'Done {des_path}')\n",
    "    return True\n",
    "\n",
    "dict_link_transfer_azure = transfer_link_azure_storage(df_vendor_transfer,dict_ven_filename)\n",
    "dict_link_transfer_sharepoint = transfer_link_sharepoint(df_vendor_transfer,dict_ven_filename)\n",
    "\n",
    "def df_line_tracker(des_path,cat_date):    \n",
    "    des_path_list = des_path.split('/')[-1].split('.')[-2].split('_')\n",
    "    cata_name = des_path_list[-1]\n",
    "    client_name = des_path_list[0]\n",
    "    promo_start = des_path_list[1]\n",
    "    if len(des_path_list) == 3:\n",
    "        promo_end = ''\n",
    "    else:\n",
    "        promo_end = des_path_list[2]\n",
    "    des_path = des_path.replace(' ','%20')\n",
    "    return pd.DataFrame([[client_name,cata_name,promo_start,promo_end,cat_date,des_path]],columns = ['CLIENT_NAME','FILE_NAME','PROMO_START','PROMO_END','RETRIEVAL_DATE','FILE_PATH'])\n",
    "\n",
    "def upload_df_catalogue_tracking(dict_link_transfer_azure,cat_date):\n",
    "    df_tracker = pd.DataFrame(columns = ['CLIENT_NAME','FILE_NAME','PROMO_START','PROMO_END','RETRIEVAL_DATE','FILE_PATH'])\n",
    "    for source_path, des_path in dict_link_transfer_azure.items():\n",
    "        # print(source_path, des_path)\n",
    "        des_path = \"https://pvnadfstorage.blob.core.windows.net/catalogue/\"+join(des_path,source_path.split('\\\\')[-1]).replace('\\\\','/')\n",
    "        # df_tracker \n",
    "        df_line = df_line_tracker(des_path,cat_date)\n",
    "        df_tracker = pd.concat([df_tracker.reset_index(drop=True),df_line.reset_index(drop=True)]).reset_index(drop=True)\n",
    "    upload_SF(df_tracker)\n",
    "    return df_tracker\n",
    "\n",
    "def upload_df_no_catalogue(cat_date,dict_ven_filename):\n",
    "    client_name_set = set()\n",
    "    for file in dict_ven_filename.keys():\n",
    "        client_name_set.add(file.split('\\\\')[-1].split('_')[0]) \n",
    "    client_name_list = list(client_name_set)\n",
    "    df_tracker_no_cata = pd.DataFrame(columns = ['CLIENT_NAME','FILE_NAME','PROMO_START','PROMO_END','RETRIEVAL_DATE','FILE_PATH'])\n",
    "    client_name_list_full = df_vendor_transfer['Retailer_name'].to_list()\n",
    "    for client_name in client_name_list_full:\n",
    "        if client_name not in client_name_list:\n",
    "            df_line = pd.DataFrame([[client_name,'No new catalogue available',None,None,cat_date,None]],columns = ['CLIENT_NAME','FILE_NAME','PROMO_START','PROMO_END','RETRIEVAL_DATE','FILE_PATH'])\n",
    "            df_tracker_no_cata = pd.concat([df_tracker_no_cata.reset_index(drop=True),df_line.reset_index(drop=True)]).reset_index(drop=True)\n",
    "    upload_SF(df_tracker_no_cata)\n",
    "    return df_tracker_no_cata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Move file to SharePoint and Azure\n",
    "copy_file_sharepoint(dict_link_transfer_sharepoint)\n",
    "copy_file_azure(dict_link_transfer_azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Update to Snowflake\n",
    "upload_df_catalogue_tracking(dict_link_transfer_azure,cat_date)\n",
    "upload_df_no_catalogue(cat_date,dict_ven_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Send email\n",
    "def send_email(cat_date):\n",
    "    ol=win32.Dispatch(\"outlook.application\")\n",
    "    olmailitem=0x0 #size of the new email\n",
    "    newmail=ol.CreateItem(olmailitem)\n",
    "    newmail.Subject= f'Ongoing Catalogue Task - Date: {cat_date}'\n",
    "    # newmail.To='tdinh@profectusgroup.com'\n",
    "    newmail.To=\n",
    "    newmail.CC=\n",
    "    newmail.BodyFormat = 2 \n",
    "    newmail.HTMLBody = f'''\n",
    "    <p style=\"font-family:Arial; font-size:10pt;\">Hi Matt & chi Hanh,<br><br>The catalogues for {cat_date} have been collected and stored on SharePoint.</p>\n",
    "\n",
    "    '''\n",
    "    newmail.Send()\n",
    "    return None\n",
    "send_email(cat_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\OneDrive - Profectus Group\\\\Ongoing Catalogue\\\\Auto\\\\Tool\\\\catalogues\\\\2024\\\\06\\\\240617'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_path_week()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
